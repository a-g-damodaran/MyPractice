{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"HpW9zDYNRElo","executionInfo":{"status":"ok","timestamp":1640598236552,"user_tz":-330,"elapsed":2294,"user":{"displayName":"Damodaran G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07652016210111082143"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import AveragePooling2D\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras.utils import to_categorical\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","\n","from tensorflow.keras.applications import NASNetMobile\n","from tensorflow.keras.applications.nasnet import preprocess_input\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","from imutils import paths\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCePqFk8RElx","executionInfo":{"status":"ok","timestamp":1640598277222,"user_tz":-330,"elapsed":403,"user":{"displayName":"Damodaran G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07652016210111082143"}},"outputId":"2c700367-34c2-45cf-d3fc-8cf145036dff"},"outputs":[{"output_type":"stream","name":"stdout","text":["No Gpu\n"]}],"source":["if not tf.test.gpu_device_name():\n","    print(\"No Gpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIRhu9zWREly","executionInfo":{"status":"ok","timestamp":1640598291110,"user_tz":-330,"elapsed":503,"user":{"displayName":"Damodaran G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07652016210111082143"}},"outputId":"7cd8c83e-2b2c-4cfa-83e6-6fbf336091fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 5301255609957811485\n","xla_global_id: -1\n","]\n"]}],"source":["from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QuOrhBMPRElz","executionInfo":{"status":"ok","timestamp":1640598298398,"user_tz":-330,"elapsed":387,"user":{"displayName":"Damodaran G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07652016210111082143"}}},"outputs":[],"source":["LR = 1e-4\n","BS = 32\n","EP = 10"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"kWI0O_DUREl0","executionInfo":{"status":"ok","timestamp":1640598301315,"user_tz":-330,"elapsed":3,"user":{"displayName":"Damodaran G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07652016210111082143"}}},"outputs":[],"source":["labels = []\n","data = [] \n","labels3 = []\n","data3 = [] "]},{"cell_type":"code","execution_count":6,"metadata":{"id":"R3E2gOweREl0","executionInfo":{"status":"ok","timestamp":1640598309916,"user_tz":-330,"elapsed":604,"user":{"displayName":"Damodaran G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07652016210111082143"}}},"outputs":[],"source":["DIR = r\"IMG_DATA\"\n","CAT = ['without_mask', 'with_mask']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jabgHYmREl1"},"outputs":[],"source":["print(\"Loading Images\")\n","for cat in CAT:\n","    path = os.path.join(DIR,cat)\n","    for img in os.listdir(path):\n","        img_path = os.path.join(path, img)\n","        image = load_img(img_path, target_size=(224, 224))\n","        \n","        image = img_to_array(image)\n","        image = preprocess_input(image)\n","\n","        data.append(image)\n","        labels.append(cat)\n","\n","print(\"Images loaded\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijDw50rzREl2"},"outputs":[],"source":["#For inceptionV3\n","print(\"Loading Images\")\n","for cat in CAT:\n","    path = os.path.join(DIR,cat)\n","    for img in os.listdir(path):\n","        img_path = os.path.join(path, img)\n","        image = load_img(img_path, target_size=(299,299))\n","        \n","        image = img_to_array(image)\n","        image = preprocess_input(image)\n","\n","        data3.append(image)\n","        labels3.append(cat)\n","print(\"Images loaded\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-IC1u8-REl3"},"outputs":[],"source":["#One hot encoding labels\n","lb = LabelBinarizer()\n","\n","labels = lb.fit_transform(labels)\n","labels = to_categorical(labels)\n","\n","data = np.array(data, dtype=\"float32\")\n","labels = np.array(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSiZR7HfREl4"},"outputs":[],"source":["#One hot encoding labels\n","lb = LabelBinarizer()\n","\n","labels3 = lb.fit_transform(labels3)\n","labels3 = to_categorical(labels3)\n","\n","data3 = np.array(data3, dtype=\"float32\")\n","labels3 = np.array(labels3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWKkousoREl5"},"outputs":[],"source":["(trainX1, testX1, trainY1, testY1) = train_test_split(data,\n","                                                  labels,\n","                                                  test_size=0.20,\n","                                                  stratify=labels,\n","                                                  random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud_tmcNjREl6"},"outputs":[],"source":["(trainX2, testX2, trainY2, testY2) = train_test_split(data,\n","                                                  labels,\n","                                                  test_size=0.20,\n","                                                  stratify=labels,\n","                                                  random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nVa4ID1REl6"},"outputs":[],"source":["(trainX3, testX3, trainY3, testY3) = train_test_split(data3,\n","                                                  labels3,\n","                                                  test_size=0.20,\n","                                                  stratify=labels3,\n","                                                  random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HaBsi2WREl7"},"outputs":[],"source":["aug = ImageDataGenerator(\n","    rotation_range=20,\n","    zoom_range=0.15,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.15,\n","    horizontal_flip=True,\n","    fill_mode=\"nearest\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7Sdx0VTREl7"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","callback = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', min_delta=0, patience=0, verbose=1, mode='auto',\n","    baseline=None, restore_best_weights=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__Z2AUqsREl8","outputId":"8be8cfa1-58d4-4f9b-a291-5c5f3c64af51"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Rj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"]}],"source":["#load into NASNetMobile\n","baseModel1 = NASNetMobile(weights=\"imagenet\", \n","                        include_top=False,\n","                        input_tensor=Input(shape=(224, 224, 3)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTY3SmUwREl9","outputId":"b806d8fe-aef8-4536-e190-316dcf60e534"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Rj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"]}],"source":["#NASNetMobile Model\n","\n","myModel1 = baseModel1.output\n","\n","myModel1 = AveragePooling2D(pool_size=(7, 7))(myModel1)\n","myModel1 = Flatten(name=\"flatten\")(myModel1)\n","myModel1 = Dense(128, activation=\"relu\")(myModel1)\n","\n","myModel1 = Dense(128, activation=\"relu\")(myModel1)\n","\n","myModel1 = Dropout(0.5)(myModel1)\n","\n","\n","myModel1 = Dense(2, activation=\"softmax\")(myModel1)\n","\n","\n","NASNetmodel = Model(inputs=baseModel1.input, outputs=myModel1)\n","\n","for layer in baseModel1.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkzfslOxREl9"},"outputs":[],"source":["opt = Adam(lr=LR, decay=LR/EP)\n","NASNetmodel.compile(loss=\"binary_crossentropy\", \n","              optimizer=opt,\n","              metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"qbbQhFhIREl9","outputId":"34b20b89-06a9-4d76-825e-ee7605417a6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Rj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch 1/10\n","276/276 [==============================] - 67s 244ms/sample - loss: 0.4871 - acc: 0.7428\n","35/35 [==============================] - 354s 10s/step - loss: 0.6380 - acc: 0.6309 - val_loss: 0.4863 - val_acc: 0.7428\n","Epoch 2/10\n","276/276 [==============================] - 58s 211ms/sample - loss: 0.3714 - acc: 0.8188\n","35/35 [==============================] - 348s 10s/step - loss: 0.4500 - acc: 0.7945 - val_loss: 0.3715 - val_acc: 0.8188\n","Epoch 3/10\n","276/276 [==============================] - 49s 179ms/sample - loss: 0.2761 - acc: 0.8841\n","35/35 [==============================] - 315s 9s/step - loss: 0.3365 - acc: 0.8764 - val_loss: 0.2769 - val_acc: 0.8841\n","Epoch 4/10\n","276/276 [==============================] - 60s 217ms/sample - loss: 0.2427 - acc: 0.8804\n","35/35 [==============================] - 347s 10s/step - loss: 0.2667 - acc: 0.8973 - val_loss: 0.2451 - val_acc: 0.8804\n","Epoch 5/10\n","276/276 [==============================] - 57s 208ms/sample - loss: 0.1754 - acc: 0.9312\n","35/35 [==============================] - 367s 10s/step - loss: 0.1958 - acc: 0.9418 - val_loss: 0.1777 - val_acc: 0.9312\n","Epoch 6/10\n","276/276 [==============================] - 58s 208ms/sample - loss: 0.1852 - acc: 0.9167\n","35/35 [==============================] - 364s 10s/step - loss: 0.1928 - acc: 0.9300 - val_loss: 0.1852 - val_acc: 0.9167\n","Epoch 7/10\n","276/276 [==============================] - 58s 212ms/sample - loss: 0.1309 - acc: 0.9457\n","35/35 [==============================] - 366s 10s/step - loss: 0.1628 - acc: 0.9427 - val_loss: 0.1341 - val_acc: 0.9457\n","Epoch 8/10\n","276/276 [==============================] - 73s 264ms/sample - loss: 0.1378 - acc: 0.9420\n","35/35 [==============================] - 503s 14s/step - loss: 0.1564 - acc: 0.9400 - val_loss: 0.1421 - val_acc: 0.9420\n","Epoch 9/10\n","276/276 [==============================] - 70s 254ms/sample - loss: 0.1393 - acc: 0.9420\n","35/35 [==============================] - 489s 14s/step - loss: 0.1252 - acc: 0.9527 - val_loss: 0.1441 - val_acc: 0.9420\n","Epoch 10/10\n","276/276 [==============================] - 58s 210ms/sample - loss: 0.1121 - acc: 0.9457\n","35/35 [==============================] - 357s 10s/step - loss: 0.1126 - acc: 0.9609 - val_loss: 0.1164 - val_acc: 0.9457\n"]}],"source":["hist1 = NASNetmodel.fit(\n","    aug.flow(trainX1, trainY1, batch_size=BS),\n","    steps_per_epoch=len(trainX1) // BS,\n","    validation_data=(testX1, testY1),\n","    validation_steps=len(testX1) // BS,\n","    epochs=EP,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4RpTz_IREl-"},"outputs":[],"source":["#Eval\n","predIdxs = NASNetmodel.predict(testX1, batch_size=BS)\n","\n","# for each image in the testing set we need to find the index of the\n","# label with corresponding largest predicted probability\n","predIdxs = np.argmax(predIdxs, axis=1)\n","\n","# show a nicely formatted classification report\n","print(classification_report(testY1.argmax(axis=1), predIdxs,target_names=lb.classes_))\n","\n","# serialize the model to disk\n","NASNetmodel.save(\"mask_detector_NASNet.model\", save_format=\"h5\")\n","\n","# plot the training loss and accuracy\n","N = EP\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), hist1.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), hist1.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), hist1.history[\"accuracy\"], label=\"train_acc\")\n","plt.plot(np.arange(0, N), hist1.history[\"val_accuracy\"], label=\"val_acc\")\n","\n","\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.savefig(\"NASNetMobile-plot.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZqcX3-kREl-"},"outputs":[],"source":["#load into MobileNetV2\n","\n","baseModel2 = MobileNetV2(weights=\"imagenet\", \n","                        include_top=False,\n","                        input_tensor=Input(shape=(224, 224, 3)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4YyqbQnREl_"},"outputs":[],"source":["#MobileNetV2 Model\n","\n","myModel2 = baseModel2.output\n","\n","myModel2 = AveragePooling2D(pool_size=(7, 7))(myModel2)\n","myModel2 = Flatten(name=\"flatten\")(myModel2)\n","myModel2 = Dense(128, activation=\"relu\")(myModel2)\n","\n","myModel2 = Dense(128, activation=\"relu\")(myModel2)\n","\n","myModel2 = Dropout(0.5)(myModel2)\n","\n","\n","myModel2 = Dense(2, activation=\"softmax\")(myModel2)\n","\n","\n","MobileNetmodel = Model(inputs=baseModel2.input, outputs=myModel2)\n","\n","for layer in baseModel2.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_zpKUXtREl_"},"outputs":[],"source":["opt = Adam(lr=LR, decay=LR/EP)\n","MobileNetmodel.compile(loss=\"binary_crossentropy\", \n","              optimizer=opt,\n","              metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaDIk0U4REmA"},"outputs":[],"source":["hist2 = MobileNetmodel.fit(\n","    aug.flow(trainX2, trainY2, batch_size=BS),\n","    steps_per_epoch=len(trainX2) // BS,\n","    validation_data=(testX2, testY2),\n","    validation_steps=len(testX2) // BS,\n","    epochs=EP,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGcZaMGnREmA"},"outputs":[],"source":["#Eval\n","predIdxs = MobileNetmodel.predict(testX2, batch_size=BS)\n","\n","# for each image in the testing set we need to find the index of the\n","# label with corresponding largest predicted probability\n","predIdxs = np.argmax(predIdxs, axis=1)\n","\n","# show a nicely formatted classification report\n","print(classification_report(testY2.argmax(axis=1), predIdxs,target_names=lb.classes_))\n","\n","# serialize the model to disk\n","MobileNetmodel.save(\"mask_detector_MobileNetV2.model\", save_format=\"h5\")\n","\n","# plot the training loss and accuracy\n","N = EP\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), hist2.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), hist2.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), hist2.history[\"accuracy\"], label=\"train_acc\")\n","plt.plot(np.arange(0, N), hist2.history[\"val_accuracy\"], label=\"val_acc\")\n","\n","\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.savefig(\"MobileNetV2-plot.png\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"id":"U6kwnexKREmA","executionInfo":{"status":"error","timestamp":1640598781508,"user_tz":-330,"elapsed":416,"user":{"displayName":"Damodaran G","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07652016210111082143"}},"outputId":"284e9cfa-64e8-4a5d-abcc-bf019e02f9d8"},"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-8982124520f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mprototxtPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"detect_face\\deploy.prototxt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mweightsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"detect_face\\res10_300x300_ssd_iter_140000.caffemodel\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mfaceNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototxtPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightsPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mmaskNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'C:\\Users\\Rj\\All PYTHON\\Edureka\\Test 7 face detection\\mask_detector_MobileNetV2.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/dnn/src/caffe/caffe_io.cpp:1121: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"detect_face\\deploy.prototxt\" in function 'ReadProtoFromTextFile'\n"]}],"source":["from tensorflow.keras.models import load_model\n","from imutils.video import VideoStream\n","import numpy as np\n","import imutils\n","import time\n","import cv2\n","import os\n","\n","def detect_predict(frame, faceNet, maskNet):\n","    # Get dimension and create blob\n","    \n","    (h, w) = frame.shape[:2]\n","    blob = cv2.dnn.blobFromImage(frame, 1.0, (299, 299),\n","        (104.0, 177.0, 123.0))\n","\n","    # pass the blob through the network and obtain the face detections\n","    faceNet.setInput(blob)\n","    detections = faceNet.forward()\n","    print(detections.shape)\n","\n","    # initialize our list of faces, their corresponding locations,\n","    # and the list of predictions from our face mask network\n","    faces = []\n","    locs = []\n","    preds = []\n","\n","    # loop over the detections\n","    for i in range(0, detections.shape[2]):\n","        # extract the confidence (i.e., probability) associated with\n","        # the detection\n","        confidence = detections[0, 0, i, 2]\n","\n","        # filter out weak detections by ensuring the confidence is\n","        # greater than the minimum confidence\n","        if confidence > 0.5:\n","            # compute the (x, y)-coordinates of the bounding box for\n","            # the object\n","            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","\n","            # ensure the bounding boxes fall within the dimensions of\n","            # the frame\n","            (startX, startY) = (max(0, startX), max(0, startY))\n","            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","            # extract the face ROI, convert it from BGR to RGB channel\n","            # ordering, resize it to 224x224, and preprocess it\n","            face = frame[startY:endY, startX:endX]\n","            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","            face = cv2.resize(face, (299, 299))\n","            face = img_to_array(face)\n","            face = preprocess_input(face)\n","\n","            # add the face and bounding boxes to their respective\n","            # lists\n","            faces.append(face)\n","            locs.append((startX, startY, endX, endY))\n","\n","    # only make a predictions if at least one face was detected\n","    if len(faces) > 0:\n","        # for faster inference we'll make batch predictions on *all*\n","        # faces at the same time rather than one-by-one predictions\n","        # in the above `for` loop\n","        faces = np.array(faces, dtype=\"float32\")\n","        preds = maskNet.predict(faces, batch_size=32)\n","\n","    # return a 2-tuple of the face locations and their corresponding\n","    # locations\n","    return (locs, preds)\n","prototxtPath = r\"detect_face\\deploy.prototxt\"\n","weightsPath = r\"detect_face\\res10_300x300_ssd_iter_140000.caffemodel\"\n","faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n","\n","maskNet = load_model(r'C:\\Users\\Rj\\All PYTHON\\Edureka\\Test 7 face detection\\mask_detector_MobileNetV2.model')\n","\n","cap = VideoStream().start()\n","\n","while True:\n","    frame = cap.read()\n","    frame = imutils.resize(frame,width=600,height=800)\n","\n","        #Pass to detector model to  id face and check mask\n","    (locs, preds) = detect_predict(frame,faceNet,maskNet)\n","    for (box, pred) in zip(locs, preds):\n","        # unpack the bounding box and predictions\n","        (startX, startY, endX, endY)= box\n","        print(\"Mask \",pred[0])\n","        (withoutMask,mask) = pred\n","\n","        # determine the class label and color we'll use to draw\n","        label = \"No Mask\" if withoutMask > mask else \"Mask\"\n","        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\n","        # include the probability in the label\n","        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","\n","        # display the label and bounding box rectangle on the output\n","        cv2.putText(frame, label, (startX, startY - 10),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","\n","    cv2.imshow(\"Frame\", frame)\n","    key = cv2.waitKey(1) & 0xFF\n","\n","    if key == ord(\"q\"):\n","        break\n","cv2.destroyAllWindows()\n","cap.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwyCQRqQREmB"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"Shubam_openCv.ipnyb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}